{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, number_of_arms: int) -> None:\n",
    "        self.number_of_arms = number_of_arms\n",
    "        self.means = np.random.normal(0, 1, number_of_arms)\n",
    "        self.optimal_arm = np.argmax(self.means)\n",
    "        self.optimal_mean = self.means[self.optimal_arm]\n",
    "\n",
    "    def play(self, arm: int) -> float:\n",
    "        mean = self.means[arm]\n",
    "        return np.random.normal(mean, 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bandit(bandit: MultiArmedBandit):\n",
    "    rewards = []\n",
    "    for arm in range(bandit.number_of_arms):\n",
    "        simulated_rewards = [bandit.play(arm) for _ in range(1000)] \n",
    "        rewards.append(simulated_rewards)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.violinplot(rewards, showmeans=True, showmedians=True)\n",
    "    plt.xlabel(\"Arm\", fontsize=12)\n",
    "    plt.ylabel(\"Reward Distribution\", fontsize=12)\n",
    "    plt.title(\"Reward Distributions of Multi-Armed Bandit Arms\", fontsize=14)\n",
    "    plt.xticks(range(1, bandit.number_of_arms + 1), [f'Arm {i}' for i in range(bandit.number_of_arms)], fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "bandit = MultiArmedBandit(10)\n",
    "plot_bandit(bandit)\n",
    "print(f\"Optimal Arm: {bandit.optimal_arm}\")\n",
    "print(f\"Optimal Mean: {bandit.optimal_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent for a baseline\n",
    "class RandomAgent:\n",
    "    def __init__(self, bandit: MultiArmedBandit) -> None:\n",
    "        self.bandit = bandit\n",
    "        self.number_of_arms = bandit.number_of_arms\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    def update(self, arm: int, reward: float) -> None:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_collect_data(agent, bandit, num_steps):\n",
    "    rewards = []\n",
    "    best_action_counts = []\n",
    "    regrets = []\n",
    "    for _ in range(num_steps):\n",
    "        action = agent.choose_action()\n",
    "        reward = bandit.play(action)\n",
    "        agent.update(action, reward)\n",
    "        rewards.append(reward)\n",
    "        best_action_counts.append(action == bandit.optimal_arm)\n",
    "        regrets.append(bandit.optimal_mean - bandit.means[action])\n",
    "    return rewards, best_action_counts, regrets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_plot_for_agents(agents, agent_names, bandit, num_steps):\n",
    "    all_rewards = []\n",
    "    all_best_action_counts = []\n",
    "    all_regrets = []\n",
    "\n",
    "    for agent in agents:\n",
    "        rewards, best_action_counts, regrets = run_and_collect_data(agent, bandit, num_steps)\n",
    "        all_rewards.append(rewards)\n",
    "        all_best_action_counts.append(best_action_counts)\n",
    "        all_regrets.append(regrets)\n",
    "    \n",
    "    average_rewards = [np.cumsum(rewards) / (np.arange(len(rewards)) + 1) for rewards in all_rewards]\n",
    "    average_best_action_counts = [np.cumsum(best_action_counts) / (np.arange(len(best_action_counts)) + 1) for best_action_counts in all_best_action_counts]\n",
    "    cumulative_regrets = [np.cumsum(regrets) for regrets in all_regrets]\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 12))\n",
    "    \n",
    "    for i, (agent, name) in enumerate(zip(agents, agent_names)):\n",
    "        ax[0].plot(average_rewards[i], label=f'{name}')\n",
    "        ax[1].plot(average_best_action_counts[i], label=f'{name}')\n",
    "        ax[2].plot(cumulative_regrets[i], label=f'{name}')\n",
    "    \n",
    "    ax[0].set_xlabel('Steps', fontsize=12)\n",
    "    ax[0].set_ylabel('Average Reward', fontsize=12)\n",
    "    ax[0].set_title('Average Reward vs Steps', fontsize=14)\n",
    "    ax[0].grid(linestyle='--', alpha=0.7)\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].set_xlabel('Steps', fontsize=12)\n",
    "    ax[1].set_ylabel('Best Action Count', fontsize=12)\n",
    "    ax[1].set_title('Best Action Count vs Steps', fontsize=14)\n",
    "    ax[1].grid(linestyle='--', alpha=0.7)\n",
    "    ax[1].legend()\n",
    "\n",
    "    ax[2].set_xlabel('Steps', fontsize=12)\n",
    "    ax[2].set_ylabel('Regret', fontsize=12)\n",
    "    ax[2].set_title('Regret vs Steps', fontsize=14)\n",
    "    ax[2].grid(linestyle='--', alpha=0.7)\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(bandit)\n",
    "agents = [random_agent]\n",
    "agent_names = ['Random Agent']\n",
    "run_and_plot_for_agents(agents, agent_names, bandit, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a agent that explore every arm N times and then exploit the best arm\n",
    "class ExploreThenExploit:\n",
    "    def __init__(self, bandit: MultiArmedBandit, N: int) -> None:\n",
    "        self.bandit = bandit\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        pass\n",
    "        \n",
    "    def update(self, arm: int, reward: float) -> None:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run ExploreThenExploit agent for different values of N and comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, bandit: MultiArmedBandit, epsilon: float=0.1) -> None:\n",
    "        pass\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def update(self, arm: int, reward: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the epsilon-greedy agent for different values of N and comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    def __init__(self, bandit: MultiArmedBandit, c: float=2) -> None:\n",
    "        pass\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def update(self, arm: int, reward: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the UCB agent for different values of c and comment on the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the epsilon greedy and UCB agents; compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new class MultiArmedBanditBernoulli that generates rewards from a Bernoulli distribution\n",
    "class MultiArmedBanditBernoulli:\n",
    "    def __init__(self, number_of_arms: int) -> None:\n",
    "        self.number_of_arms = number_of_arms\n",
    "        self.means = np.random.uniform(0, 1, number_of_arms)\n",
    "        self.optimal_arm = np.argmax(self.means)\n",
    "        self.optimal_mean = self.means[self.optimal_arm]\n",
    "\n",
    "    def play(self, arm: int) -> float:\n",
    "        pass\n",
    "    \n",
    "bandit_bernoulli = MultiArmedBanditBernoulli(10)\n",
    "print(bandit_bernoulli.means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the Thompson Sampling algorithm\n",
    "\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, bandit: MultiArmedBandit) -> None:\n",
    "        self.bandit = bandit\n",
    "        self.number_of_arms = bandit.number_of_arms\n",
    "        self.alpha = np.ones(self.number_of_arms)\n",
    "        self.beta = np.ones(self.number_of_arms)\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def update(self, action: int, reward: float) -> None:\n",
    "        pass\n",
    "\n",
    "    def plot_beta_distribution(self, arms= None):\n",
    "        '''Plot the beta distribution of the arms'''\n",
    "        if arms is None:\n",
    "            arms = range(self.number_of_arms)\n",
    "        x = np.linspace(0, 1, 1000)\n",
    "        for i in arms:\n",
    "            y = np.exp(np.log(x) * (self.alpha[i] - 1) + np.log(1 - x) * (self.beta[i] - 1))\n",
    "            plt.plot(x, y, label=f'Arm {i}')\n",
    "        plt.xlabel('p')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Beta Distribution for Each Arm')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the Thompson Sampling agent and compare the results with other agents. Plot the beta distributions at the end of the learning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
